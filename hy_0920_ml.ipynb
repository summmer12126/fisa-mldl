{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I9_jA88fivdv"
      },
      "source": [
        "#soft voting\n",
        "- randomforest\n",
        "- xgbm\n",
        "- lgbm\n",
        "- weight 1,1,1\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VTFpRwrHi7LO",
        "outputId": "fda79769-71c1-47b7-a9f7-8fe009ca4c44"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: scikit-learn in /Users/summerlee/.pyenv/versions/py_3_11/lib/python3.11/site-packages (1.5.2)\n",
            "Requirement already satisfied: pandas in /Users/summerlee/.pyenv/versions/py_3_11/lib/python3.11/site-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /Users/summerlee/.pyenv/versions/py_3_11/lib/python3.11/site-packages (2.1.1)\n",
            "Requirement already satisfied: matplotlib in /Users/summerlee/.pyenv/versions/py_3_11/lib/python3.11/site-packages (3.9.2)\n",
            "Requirement already satisfied: seaborn in /Users/summerlee/.pyenv/versions/py_3_11/lib/python3.11/site-packages (0.13.2)\n",
            "Requirement already satisfied: Optuna in /Users/summerlee/.pyenv/versions/py_3_11/lib/python3.11/site-packages (4.0.0)\n",
            "Requirement already satisfied: xgboost in /Users/summerlee/.pyenv/versions/py_3_11/lib/python3.11/site-packages (2.1.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /Users/summerlee/.pyenv/versions/py_3_11/lib/python3.11/site-packages (from scikit-learn) (1.14.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /Users/summerlee/.pyenv/versions/py_3_11/lib/python3.11/site-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/summerlee/.pyenv/versions/py_3_11/lib/python3.11/site-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/summerlee/.pyenv/versions/py_3_11/lib/python3.11/site-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /Users/summerlee/.pyenv/versions/py_3_11/lib/python3.11/site-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /Users/summerlee/.pyenv/versions/py_3_11/lib/python3.11/site-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /Users/summerlee/.pyenv/versions/py_3_11/lib/python3.11/site-packages (from matplotlib) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /Users/summerlee/.pyenv/versions/py_3_11/lib/python3.11/site-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /Users/summerlee/.pyenv/versions/py_3_11/lib/python3.11/site-packages (from matplotlib) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/summerlee/.pyenv/versions/py_3_11/lib/python3.11/site-packages (from matplotlib) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /Users/summerlee/.pyenv/versions/py_3_11/lib/python3.11/site-packages (from matplotlib) (24.1)\n",
            "Requirement already satisfied: pillow>=8 in /Users/summerlee/.pyenv/versions/py_3_11/lib/python3.11/site-packages (from matplotlib) (9.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /Users/summerlee/.pyenv/versions/py_3_11/lib/python3.11/site-packages (from matplotlib) (3.1.4)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /Users/summerlee/.pyenv/versions/py_3_11/lib/python3.11/site-packages (from Optuna) (1.13.2)\n",
            "Requirement already satisfied: colorlog in /Users/summerlee/.pyenv/versions/py_3_11/lib/python3.11/site-packages (from Optuna) (6.8.2)\n",
            "Requirement already satisfied: sqlalchemy>=1.3.0 in /Users/summerlee/.pyenv/versions/py_3_11/lib/python3.11/site-packages (from Optuna) (1.4.46)\n",
            "Requirement already satisfied: tqdm in /Users/summerlee/.pyenv/versions/py_3_11/lib/python3.11/site-packages (from Optuna) (4.66.5)\n",
            "Requirement already satisfied: PyYAML in /Users/summerlee/.pyenv/versions/py_3_11/lib/python3.11/site-packages (from Optuna) (6.0.2)\n",
            "Requirement already satisfied: Mako in /Users/summerlee/.pyenv/versions/py_3_11/lib/python3.11/site-packages (from alembic>=1.5.0->Optuna) (1.3.5)\n",
            "Requirement already satisfied: typing-extensions>=4 in /Users/summerlee/.pyenv/versions/py_3_11/lib/python3.11/site-packages (from alembic>=1.5.0->Optuna) (4.12.2)\n",
            "Requirement already satisfied: six>=1.5 in /Users/summerlee/.pyenv/versions/py_3_11/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /Users/summerlee/.pyenv/versions/py_3_11/lib/python3.11/site-packages (from Mako->alembic>=1.5.0->Optuna) (2.1.5)\n"
          ]
        }
      ],
      "source": [
        "! pip install  scikit-learn pandas numpy matplotlib seaborn Optuna xgboost\n",
        "# pycaret\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: lightgbm in /Users/summerlee/.pyenv/versions/py_3_11/lib/python3.11/site-packages (4.5.0)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /Users/summerlee/.pyenv/versions/py_3_11/lib/python3.11/site-packages (from lightgbm) (2.1.1)\n",
            "Requirement already satisfied: scipy in /Users/summerlee/.pyenv/versions/py_3_11/lib/python3.11/site-packages (from lightgbm) (1.14.1)\n"
          ]
        }
      ],
      "source": [
        "! pip install --no-binary :all: lightgbm\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oKCrTCqyoZpR",
        "outputId": "fccbf813-3d24-4469-cc89-aef5f3d364b8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting lightgbm==3.3.2\n",
            "  Using cached lightgbm-3.3.2.tar.gz (1.5 MB)\n",
            "  Installing build dependencies ... \u001b[?25ldone\n",
            "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
            "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25hCollecting wheel (from lightgbm==3.3.2)\n",
            "  Using cached wheel-0.44.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: numpy in /Users/summerlee/.pyenv/versions/py_3_11/lib/python3.11/site-packages (from lightgbm==3.3.2) (2.1.1)\n",
            "Requirement already satisfied: scipy in /Users/summerlee/.pyenv/versions/py_3_11/lib/python3.11/site-packages (from lightgbm==3.3.2) (1.14.1)\n",
            "Requirement already satisfied: scikit-learn!=0.22.0 in /Users/summerlee/.pyenv/versions/py_3_11/lib/python3.11/site-packages (from lightgbm==3.3.2) (1.5.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /Users/summerlee/.pyenv/versions/py_3_11/lib/python3.11/site-packages (from scikit-learn!=0.22.0->lightgbm==3.3.2) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/summerlee/.pyenv/versions/py_3_11/lib/python3.11/site-packages (from scikit-learn!=0.22.0->lightgbm==3.3.2) (3.5.0)\n",
            "Using cached wheel-0.44.0-py3-none-any.whl (67 kB)\n",
            "Building wheels for collected packages: lightgbm\n",
            "  Building wheel for lightgbm (pyproject.toml) ... \u001b[?25lerror\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mBuilding wheel for lightgbm \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mpyproject.toml\u001b[0m\u001b[1;32m)\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m \u001b[31m[96 lines of output]\u001b[0m\n",
            "  \u001b[31m   \u001b[0m INFO:root:running bdist_wheel\n",
            "  \u001b[31m   \u001b[0m INFO:root:running build\n",
            "  \u001b[31m   \u001b[0m INFO:root:running build_py\n",
            "  \u001b[31m   \u001b[0m INFO:root:creating build/lib/lightgbm\n",
            "  \u001b[31m   \u001b[0m INFO:root:copying lightgbm/callback.py -> build/lib/lightgbm\n",
            "  \u001b[31m   \u001b[0m INFO:root:copying lightgbm/compat.py -> build/lib/lightgbm\n",
            "  \u001b[31m   \u001b[0m INFO:root:copying lightgbm/plotting.py -> build/lib/lightgbm\n",
            "  \u001b[31m   \u001b[0m INFO:root:copying lightgbm/__init__.py -> build/lib/lightgbm\n",
            "  \u001b[31m   \u001b[0m INFO:root:copying lightgbm/engine.py -> build/lib/lightgbm\n",
            "  \u001b[31m   \u001b[0m INFO:root:copying lightgbm/dask.py -> build/lib/lightgbm\n",
            "  \u001b[31m   \u001b[0m INFO:root:copying lightgbm/basic.py -> build/lib/lightgbm\n",
            "  \u001b[31m   \u001b[0m INFO:root:copying lightgbm/libpath.py -> build/lib/lightgbm\n",
            "  \u001b[31m   \u001b[0m INFO:root:copying lightgbm/sklearn.py -> build/lib/lightgbm\n",
            "  \u001b[31m   \u001b[0m INFO:root:running egg_info\n",
            "  \u001b[31m   \u001b[0m INFO:root:writing lightgbm.egg-info/PKG-INFO\n",
            "  \u001b[31m   \u001b[0m INFO:root:writing dependency_links to lightgbm.egg-info/dependency_links.txt\n",
            "  \u001b[31m   \u001b[0m INFO:root:writing requirements to lightgbm.egg-info/requires.txt\n",
            "  \u001b[31m   \u001b[0m INFO:root:writing top-level names to lightgbm.egg-info/top_level.txt\n",
            "  \u001b[31m   \u001b[0m INFO:root:reading manifest file 'lightgbm.egg-info/SOURCES.txt'\n",
            "  \u001b[31m   \u001b[0m INFO:root:reading manifest template 'MANIFEST.in'\n",
            "  \u001b[31m   \u001b[0m WARNING:root:no previously-included directories found matching 'build'\n",
            "  \u001b[31m   \u001b[0m WARNING:root:warning: no files found matching '*.so' under directory 'lightgbm'\n",
            "  \u001b[31m   \u001b[0m WARNING:root:warning: no files found matching '*.so' under directory 'compile'\n",
            "  \u001b[31m   \u001b[0m WARNING:root:warning: no files found matching '*.dll' under directory 'compile/Release'\n",
            "  \u001b[31m   \u001b[0m WARNING:root:warning: no files found matching '*.dll' under directory 'compile/windows/x64/DLL'\n",
            "  \u001b[31m   \u001b[0m WARNING:root:warning: no previously-included files matching '*.py[co]' found anywhere in distribution\n",
            "  \u001b[31m   \u001b[0m WARNING:root:warning: no previously-included files found matching 'compile/external_libs/compute/.git'\n",
            "  \u001b[31m   \u001b[0m INFO:root:adding license file 'LICENSE'\n",
            "  \u001b[31m   \u001b[0m INFO:root:writing manifest file 'lightgbm.egg-info/SOURCES.txt'\n",
            "  \u001b[31m   \u001b[0m INFO:root:copying lightgbm/VERSION.txt -> build/lib/lightgbm\n",
            "  \u001b[31m   \u001b[0m INFO:wheel:installing to build/bdist.macosx-14.6-arm64/wheel\n",
            "  \u001b[31m   \u001b[0m INFO:root:running install\n",
            "  \u001b[31m   \u001b[0m INFO:LightGBM:Starting to compile the library.\n",
            "  \u001b[31m   \u001b[0m INFO:LightGBM:Starting to compile with CMake.\n",
            "  \u001b[31m   \u001b[0m Traceback (most recent call last):\n",
            "  \u001b[31m   \u001b[0m   File \"<string>\", line 95, in silent_call\n",
            "  \u001b[31m   \u001b[0m   File \"/Users/summerlee/.pyenv/versions/3.11.10/lib/python3.11/subprocess.py\", line 408, in check_call\n",
            "  \u001b[31m   \u001b[0m     retcode = call(*popenargs, **kwargs)\n",
            "  \u001b[31m   \u001b[0m               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  \u001b[31m   \u001b[0m   File \"/Users/summerlee/.pyenv/versions/3.11.10/lib/python3.11/subprocess.py\", line 389, in call\n",
            "  \u001b[31m   \u001b[0m     with Popen(*popenargs, **kwargs) as p:\n",
            "  \u001b[31m   \u001b[0m          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  \u001b[31m   \u001b[0m   File \"/Users/summerlee/.pyenv/versions/3.11.10/lib/python3.11/subprocess.py\", line 1026, in __init__\n",
            "  \u001b[31m   \u001b[0m     self._execute_child(args, executable, preexec_fn, close_fds,\n",
            "  \u001b[31m   \u001b[0m   File \"/Users/summerlee/.pyenv/versions/3.11.10/lib/python3.11/subprocess.py\", line 1955, in _execute_child\n",
            "  \u001b[31m   \u001b[0m     raise child_exception_type(errno_num, err_msg, err_filename)\n",
            "  \u001b[31m   \u001b[0m FileNotFoundError: [Errno 2] No such file or directory: 'cmake'\n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m During handling of the above exception, another exception occurred:\n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m Traceback (most recent call last):\n",
            "  \u001b[31m   \u001b[0m   File \"/Users/summerlee/.pyenv/versions/py_3_11/lib/python3.11/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\", line 353, in <module>\n",
            "  \u001b[31m   \u001b[0m     main()\n",
            "  \u001b[31m   \u001b[0m   File \"/Users/summerlee/.pyenv/versions/py_3_11/lib/python3.11/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\", line 335, in main\n",
            "  \u001b[31m   \u001b[0m     json_out['return_val'] = hook(**hook_input['kwargs'])\n",
            "  \u001b[31m   \u001b[0m                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  \u001b[31m   \u001b[0m   File \"/Users/summerlee/.pyenv/versions/py_3_11/lib/python3.11/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\", line 251, in build_wheel\n",
            "  \u001b[31m   \u001b[0m     return _build_backend().build_wheel(wheel_directory, config_settings,\n",
            "  \u001b[31m   \u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  \u001b[31m   \u001b[0m   File \"/private/var/folders/0h/6t5rrrkd413d6v0z43zmchmm0000gn/T/pip-build-env-zyjkba_i/overlay/lib/python3.11/site-packages/setuptools/build_meta.py\", line 421, in build_wheel\n",
            "  \u001b[31m   \u001b[0m     return self._build_with_temp_dir(\n",
            "  \u001b[31m   \u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  \u001b[31m   \u001b[0m   File \"/private/var/folders/0h/6t5rrrkd413d6v0z43zmchmm0000gn/T/pip-build-env-zyjkba_i/overlay/lib/python3.11/site-packages/setuptools/build_meta.py\", line 403, in _build_with_temp_dir\n",
            "  \u001b[31m   \u001b[0m     self.run_setup()\n",
            "  \u001b[31m   \u001b[0m   File \"/private/var/folders/0h/6t5rrrkd413d6v0z43zmchmm0000gn/T/pip-build-env-zyjkba_i/overlay/lib/python3.11/site-packages/setuptools/build_meta.py\", line 503, in run_setup\n",
            "  \u001b[31m   \u001b[0m     super().run_setup(setup_script=setup_script)\n",
            "  \u001b[31m   \u001b[0m   File \"/private/var/folders/0h/6t5rrrkd413d6v0z43zmchmm0000gn/T/pip-build-env-zyjkba_i/overlay/lib/python3.11/site-packages/setuptools/build_meta.py\", line 318, in run_setup\n",
            "  \u001b[31m   \u001b[0m     exec(code, locals())\n",
            "  \u001b[31m   \u001b[0m   File \"<string>\", line 334, in <module>\n",
            "  \u001b[31m   \u001b[0m   File \"/private/var/folders/0h/6t5rrrkd413d6v0z43zmchmm0000gn/T/pip-build-env-zyjkba_i/overlay/lib/python3.11/site-packages/setuptools/__init__.py\", line 117, in setup\n",
            "  \u001b[31m   \u001b[0m     return distutils.core.setup(**attrs)\n",
            "  \u001b[31m   \u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  \u001b[31m   \u001b[0m   File \"/private/var/folders/0h/6t5rrrkd413d6v0z43zmchmm0000gn/T/pip-build-env-zyjkba_i/overlay/lib/python3.11/site-packages/setuptools/_distutils/core.py\", line 183, in setup\n",
            "  \u001b[31m   \u001b[0m     return run_commands(dist)\n",
            "  \u001b[31m   \u001b[0m            ^^^^^^^^^^^^^^^^^^\n",
            "  \u001b[31m   \u001b[0m   File \"/private/var/folders/0h/6t5rrrkd413d6v0z43zmchmm0000gn/T/pip-build-env-zyjkba_i/overlay/lib/python3.11/site-packages/setuptools/_distutils/core.py\", line 199, in run_commands\n",
            "  \u001b[31m   \u001b[0m     dist.run_commands()\n",
            "  \u001b[31m   \u001b[0m   File \"/private/var/folders/0h/6t5rrrkd413d6v0z43zmchmm0000gn/T/pip-build-env-zyjkba_i/overlay/lib/python3.11/site-packages/setuptools/_distutils/dist.py\", line 954, in run_commands\n",
            "  \u001b[31m   \u001b[0m     self.run_command(cmd)\n",
            "  \u001b[31m   \u001b[0m   File \"/private/var/folders/0h/6t5rrrkd413d6v0z43zmchmm0000gn/T/pip-build-env-zyjkba_i/overlay/lib/python3.11/site-packages/setuptools/dist.py\", line 950, in run_command\n",
            "  \u001b[31m   \u001b[0m     super().run_command(command)\n",
            "  \u001b[31m   \u001b[0m   File \"/private/var/folders/0h/6t5rrrkd413d6v0z43zmchmm0000gn/T/pip-build-env-zyjkba_i/overlay/lib/python3.11/site-packages/setuptools/_distutils/dist.py\", line 973, in run_command\n",
            "  \u001b[31m   \u001b[0m     cmd_obj.run()\n",
            "  \u001b[31m   \u001b[0m   File \"/private/var/folders/0h/6t5rrrkd413d6v0z43zmchmm0000gn/T/pip-build-env-zyjkba_i/overlay/lib/python3.11/site-packages/setuptools/_vendor/wheel/bdist_wheel.py\", line 403, in run\n",
            "  \u001b[31m   \u001b[0m     self.run_command(\"install\")\n",
            "  \u001b[31m   \u001b[0m   File \"/private/var/folders/0h/6t5rrrkd413d6v0z43zmchmm0000gn/T/pip-build-env-zyjkba_i/overlay/lib/python3.11/site-packages/setuptools/_distutils/cmd.py\", line 316, in run_command\n",
            "  \u001b[31m   \u001b[0m     self.distribution.run_command(command)\n",
            "  \u001b[31m   \u001b[0m   File \"/private/var/folders/0h/6t5rrrkd413d6v0z43zmchmm0000gn/T/pip-build-env-zyjkba_i/overlay/lib/python3.11/site-packages/setuptools/dist.py\", line 950, in run_command\n",
            "  \u001b[31m   \u001b[0m     super().run_command(command)\n",
            "  \u001b[31m   \u001b[0m   File \"/private/var/folders/0h/6t5rrrkd413d6v0z43zmchmm0000gn/T/pip-build-env-zyjkba_i/overlay/lib/python3.11/site-packages/setuptools/_distutils/dist.py\", line 973, in run_command\n",
            "  \u001b[31m   \u001b[0m     cmd_obj.run()\n",
            "  \u001b[31m   \u001b[0m   File \"<string>\", line 248, in run\n",
            "  \u001b[31m   \u001b[0m   File \"<string>\", line 198, in compile_cpp\n",
            "  \u001b[31m   \u001b[0m   File \"<string>\", line 99, in silent_call\n",
            "  \u001b[31m   \u001b[0m Exception: Please install CMake and all required dependencies first\n",
            "  \u001b[31m   \u001b[0m The full version of error log was saved into /Users/summerlee/LightGBM_compilation.log\n",
            "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "\u001b[?25h\u001b[31m  ERROR: Failed building wheel for lightgbm\u001b[0m\u001b[31m\n",
            "\u001b[0mFailed to build lightgbm\n",
            "\u001b[31mERROR: ERROR: Failed to build installable wheels for some pyproject.toml based projects (lightgbm)\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "! pip install lightgbm==3.3.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 461
        },
        "id": "OGxkBiwXjNIl",
        "outputId": "1d2f85ad-c0fa-45ae-d7fc-7f5321bc14fa"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/Colab Notebooks/hy-0919-ml/bank.csv'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[9], line 13\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# data import\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m df\u001b[38;5;241m=\u001b[39m\u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/content/drive/MyDrive/Colab Notebooks/hy-0919-ml/bank.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m df\n",
            "File \u001b[0;32m~/.pyenv/versions/3.11.10/envs/py_3_11/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.pyenv/versions/3.11.10/envs/py_3_11/lib/python3.11/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
            "File \u001b[0;32m~/.pyenv/versions/3.11.10/envs/py_3_11/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.pyenv/versions/3.11.10/envs/py_3_11/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
            "File \u001b[0;32m~/.pyenv/versions/3.11.10/envs/py_3_11/lib/python3.11/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/Colab Notebooks/hy-0919-ml/bank.csv'"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
        "from sklearn.metrics import roc_auc_score, recall_score, confusion_matrix\n",
        "from xgboost import XGBClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "# data import\n",
        "df=pd.read_csv('/content/drive/MyDrive/Colab Notebooks/hy-0919-ml/bank.csv')\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SecoY6qU18z8",
        "outputId": "5032a830-a722-40fe-8004-da6a6951280c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AUC: 0.8003\n",
            "Recall: 0.6289\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# 데이터 준비 (예시 데이터를 df라고 가정)\n",
        "X = df.drop(['deposit', 'duration'], axis=1)\n",
        "df['deposit'] = df['deposit'].replace({'yes': 1, 'no': 0})\n",
        "y = df['deposit']\n",
        "\n",
        "\n",
        "# 범주형 변수 원-핫 인코딩\n",
        "X = pd.get_dummies(X, drop_first=True)\n",
        "\n",
        "# 데이터 분할\n",
        "# 데이터를 train+validation과 test로 분할\n",
        "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# train+validation을 다시 train과 validation으로 분할\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# 모델 생성\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "# gbm = GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
        "xgb = XGBClassifier(n_estimators=100, random_state=42)\n",
        "lgbm = LGBMClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "# Voting Classifier 생성\n",
        "voting_clf = VotingClassifier([\n",
        "    ('rf', rf),\n",
        "    ('xgb', xgb),\n",
        "    ('lgbm', lgbm)\n",
        "], voting='soft', weights=[1, 1, 1])\n",
        "\n",
        "# 모델 학습\n",
        "voting_clf.fit(X_train, y_train)\n",
        "\n",
        "# 예측\n",
        "y_pred = voting_clf.predict(X_test)\n",
        "y_pred_proba = voting_clf.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# AUC와 Recall 계산\n",
        "auc = roc_auc_score(y_test, y_pred_proba)\n",
        "recall = recall_score(y_test, y_pred, pos_label=1)\n",
        "\n",
        "print(f\"AUC: {auc:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "\n",
        "\n",
        "# ## 특성 중요도 (Random Forest 기준)\n",
        "# importances = voting_clf.named_estimators_['rf'].feature_importances_\n",
        "# indices = np.argsort(importances)[::-1]\n",
        "# feature_names = X.columns\n",
        "\n",
        "# plt.figure(figsize=(10, 6))\n",
        "# plt.title(\"Feature Importances\")\n",
        "# plt.bar(range(X.shape[1]), importances[indices])\n",
        "# plt.xticks(range(X.shape[1]), [feature_names[i] for i in indices], rotation=90)\n",
        "# plt.tight_layout()\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yDMwDbB3omWQ"
      },
      "source": [
        "##gridsearch 사용\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ee17VT35v3Ds",
        "outputId": "659435b4-cad6-410d-fdac-9c847b094227"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: lightgbm==3.3.2 in /usr/local/lib/python3.10/dist-packages (3.3.2)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from lightgbm==3.3.2) (0.44.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from lightgbm==3.3.2) (1.26.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from lightgbm==3.3.2) (1.11.4)\n",
            "Requirement already satisfied: scikit-learn!=0.22.0 in /usr/local/lib/python3.10/dist-packages (from lightgbm==3.3.2) (1.3.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn!=0.22.0->lightgbm==3.3.2) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn!=0.22.0->lightgbm==3.3.2) (3.5.0)\n"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "qD91Sd_ojlpX",
        "outputId": "461632af-e703-4fe8-844a-cf94ddad6428"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'xgboost'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[1], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mensemble\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RandomForestClassifier, VotingClassifier\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m roc_auc_score, recall_score, make_scorer\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mxgboost\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m XGBClassifier\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlightgbm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LGBMClassifier\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlightgbm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m early_stopping\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'xgboost'"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
        "from sklearn.metrics import roc_auc_score, recall_score, make_scorer\n",
        "from xgboost import XGBClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "from lightgbm import early_stopping\n",
        "\n",
        "\n",
        "# 데이터 준비\n",
        "X = df.drop(['deposit', 'duration'], axis=1)\n",
        "df['deposit'] = df['deposit'].replace({'yes': 1, 'no': 0})\n",
        "y = df['deposit']\n",
        "\n",
        "# 범주형 변수 원-핫 인코딩\n",
        "X = pd.get_dummies(X, drop_first=True)\n",
        "\n",
        "# 데이터를 train+validation과 test로 분할\n",
        "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# train+validation을 다시 train과 validation으로 분할\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.2, random_state=42)\n",
        "\n",
        "# 커스텀 스코어 함수 (AUC와 Recall의 조화 평균)\n",
        "def custom_score(y_true, y_pred_proba):\n",
        "    auc = roc_auc_score(y_true, y_pred_proba)\n",
        "    recall = recall_score(y_true, y_pred_proba > 0.5, pos_label=1)\n",
        "    return 2 * (auc * recall) / (auc + recall)\n",
        "\n",
        "custom_scorer = make_scorer(custom_score, needs_proba=True)\n",
        "\n",
        "# 각 모델에 대한 파라미터 그리드 정의\n",
        "param_grids = {\n",
        "    'rf': {\n",
        "        'n_estimators': [100, 200],\n",
        "        'max_depth': [None, 10, 20],\n",
        "        'min_samples_split': [2, 5]\n",
        "    },\n",
        "    'xgb': {\n",
        "        'n_estimators': [100, 200],\n",
        "        'learning_rate': [0.01, 0.1],\n",
        "        'max_depth': [3, 5, 7]\n",
        "    },\n",
        "    'lgbm': {\n",
        "        'learning_rate': [0.01, 0.1],\n",
        "        'num_leaves': [31, 63]\n",
        "    }\n",
        "}\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from lightgbm import LGBMClassifier\n",
        "\n",
        "best_models = {}\n",
        "\n",
        "for name, model in [('rf', RandomForestClassifier(random_state=42)),\n",
        "                    ('xgb', XGBClassifier(random_state=42)),\n",
        "                    ('lgbm', LGBMClassifier(random_state=42))]:\n",
        "    print(f\"Performing GridSearch for {name}...\")\n",
        "\n",
        "    if name == 'lgbm':\n",
        "        param_grids[name]['n_estimators'] = [1000]  # Set a high number of estimators\n",
        "        # Use GridSearchCV for parameter tuning\n",
        "        grid_search = GridSearchCV(model, param_grids[name], cv=3, scoring=custom_scorer, n_jobs=-1)\n",
        "        grid_search.fit(X_train, y_train)\n",
        "\n",
        "        # Extract the best model and perform early stopping separately\n",
        "        best_lgbm_model = grid_search.best_estimator_\n",
        "        best_lgbm_model.fit(X_train, y_train,\n",
        "                            eval_set=[(X_val, y_val)],\n",
        "                            eval_metric='auc',\n",
        "                            early_stopping_rounds=50,\n",
        "                            verbose=False)\n",
        "\n",
        "        best_models[name] = best_lgbm_model\n",
        "    else:\n",
        "        grid_search = GridSearchCV(model, param_grids[name], cv=3, scoring=custom_scorer, n_jobs=-1)\n",
        "        grid_search.fit(X_train, y_train)\n",
        "        best_models[name] = grid_search.best_estimator_\n",
        "\n",
        "    print(f\"Best parameters for {name}: {grid_search.best_params_}\")\n",
        "    print(f\"Best score for {name}: {grid_search.best_score_:.4f}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Voting Classifier 생성\n",
        "voting_clf = VotingClassifier([\n",
        "    ('rf', best_models['rf']),\n",
        "    ('xgb', best_models['xgb']),\n",
        "    ('lgbm', best_models['lgbm'])\n",
        "], voting='soft', weights=[1, 1, 1])\n",
        "\n",
        "# 모델 학습 (train+validation 데이터 사용)\n",
        "voting_clf.fit(X_train_val, y_train_val)\n",
        "\n",
        "# 예측\n",
        "y_pred = voting_clf.predict(X_test)\n",
        "y_pred_proba = voting_clf.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# AUC와 Recall 계산\n",
        "auc = roc_auc_score(y_test, y_pred_proba)\n",
        "recall = recall_score(y_test, y_pred, pos_label=1)\n",
        "\n",
        "print(f\"Final AUC: {auc:.4f}\")\n",
        "print(f\"Final Recall: {recall:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_brIaj9iuVP"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7_f9fFlxzf9"
      },
      "source": [
        "## optuna 사용"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tmSJNQHUx1TV",
        "outputId": "fbbff4ab-184e-49c2-c92f-8b04f3ece88e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-09-19 16:27:59,417] Using an existing study with name 'bank_marketing_voting_clf' instead of creating a new one.\n",
            "<ipython-input-12-0edf83b10153>:33: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('xgb_learning_rate', 1e-3, 0.1),\n",
            "<ipython-input-12-0edf83b10153>:40: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('lgbm_learning_rate', 1e-3, 0.1),\n",
            "[I 2024-09-19 16:28:12,907] Trial 84 finished with values: [0.8028939284089502, 0.6204311152764761] and parameters: {'rf_n_estimators': 248, 'rf_max_depth': 5, 'rf_min_samples_split': 8, 'xgb_n_estimators': 133, 'xgb_learning_rate': 0.093104403747103, 'xgb_max_depth': 9, 'lgbm_n_estimators': 387, 'lgbm_learning_rate': 0.02819691275006149, 'lgbm_num_leaves': 93}.\n",
            "<ipython-input-12-0edf83b10153>:33: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('xgb_learning_rate', 1e-3, 0.1),\n",
            "<ipython-input-12-0edf83b10153>:40: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('lgbm_learning_rate', 1e-3, 0.1),\n",
            "[I 2024-09-19 16:28:25,821] Trial 85 finished with values: [0.8049202570165948, 0.6082474226804123] and parameters: {'rf_n_estimators': 193, 'rf_max_depth': 14, 'rf_min_samples_split': 4, 'xgb_n_estimators': 167, 'xgb_learning_rate': 0.03941058677846209, 'xgb_max_depth': 10, 'lgbm_n_estimators': 385, 'lgbm_learning_rate': 0.007164503642525063, 'lgbm_num_leaves': 70}.\n",
            "<ipython-input-12-0edf83b10153>:33: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('xgb_learning_rate', 1e-3, 0.1),\n",
            "<ipython-input-12-0edf83b10153>:40: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('lgbm_learning_rate', 1e-3, 0.1),\n",
            "[I 2024-09-19 16:28:50,946] Trial 86 finished with values: [0.8003073653548446, 0.6326148078725399] and parameters: {'rf_n_estimators': 298, 'rf_max_depth': 23, 'rf_min_samples_split': 10, 'xgb_n_estimators': 267, 'xgb_learning_rate': 0.019627014425181187, 'xgb_max_depth': 7, 'lgbm_n_estimators': 840, 'lgbm_learning_rate': 0.06741163809401653, 'lgbm_num_leaves': 75}.\n",
            "<ipython-input-12-0edf83b10153>:33: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('xgb_learning_rate', 1e-3, 0.1),\n",
            "<ipython-input-12-0edf83b10153>:40: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('lgbm_learning_rate', 1e-3, 0.1),\n",
            "[I 2024-09-19 16:29:34,170] Trial 87 finished with values: [0.8073549057086042, 0.619493908153702] and parameters: {'rf_n_estimators': 279, 'rf_max_depth': 19, 'rf_min_samples_split': 10, 'xgb_n_estimators': 119, 'xgb_learning_rate': 0.05328113339271193, 'xgb_max_depth': 6, 'lgbm_n_estimators': 416, 'lgbm_learning_rate': 0.007451861088267603, 'lgbm_num_leaves': 127}.\n",
            "<ipython-input-12-0edf83b10153>:33: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('xgb_learning_rate', 1e-3, 0.1),\n",
            "<ipython-input-12-0edf83b10153>:40: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('lgbm_learning_rate', 1e-3, 0.1),\n",
            "[I 2024-09-19 16:30:06,146] Trial 88 finished with values: [0.8021962476348783, 0.5885660731021556] and parameters: {'rf_n_estimators': 298, 'rf_max_depth': 11, 'rf_min_samples_split': 10, 'xgb_n_estimators': 144, 'xgb_learning_rate': 0.00705354188051048, 'xgb_max_depth': 4, 'lgbm_n_estimators': 907, 'lgbm_learning_rate': 0.004851642206955716, 'lgbm_num_leaves': 43}.\n",
            "<ipython-input-12-0edf83b10153>:33: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('xgb_learning_rate', 1e-3, 0.1),\n",
            "<ipython-input-12-0edf83b10153>:40: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('lgbm_learning_rate', 1e-3, 0.1),\n",
            "[I 2024-09-19 16:30:28,791] Trial 89 finished with values: [0.8011272206423485, 0.6157450796626054] and parameters: {'rf_n_estimators': 120, 'rf_max_depth': 10, 'rf_min_samples_split': 4, 'xgb_n_estimators': 119, 'xgb_learning_rate': 0.04553161280542373, 'xgb_max_depth': 3, 'lgbm_n_estimators': 561, 'lgbm_learning_rate': 0.04059040270910901, 'lgbm_num_leaves': 72}.\n",
            "<ipython-input-12-0edf83b10153>:33: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('xgb_learning_rate', 1e-3, 0.1),\n",
            "<ipython-input-12-0edf83b10153>:40: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('lgbm_learning_rate', 1e-3, 0.1),\n",
            "[I 2024-09-19 16:30:42,127] Trial 90 finished with values: [0.799706941923702, 0.5857544517338332] and parameters: {'rf_n_estimators': 120, 'rf_max_depth': 20, 'rf_min_samples_split': 10, 'xgb_n_estimators': 119, 'xgb_learning_rate': 0.009602342879082898, 'xgb_max_depth': 8, 'lgbm_n_estimators': 561, 'lgbm_learning_rate': 0.0016298254224030588, 'lgbm_num_leaves': 68}.\n",
            "<ipython-input-12-0edf83b10153>:33: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('xgb_learning_rate', 1e-3, 0.1),\n",
            "<ipython-input-12-0edf83b10153>:40: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('lgbm_learning_rate', 1e-3, 0.1),\n",
            "[I 2024-09-19 16:31:04,294] Trial 91 finished with values: [0.8058831850895652, 0.6213683223992502] and parameters: {'rf_n_estimators': 283, 'rf_max_depth': 22, 'rf_min_samples_split': 3, 'xgb_n_estimators': 299, 'xgb_learning_rate': 0.093104403747103, 'xgb_max_depth': 4, 'lgbm_n_estimators': 579, 'lgbm_learning_rate': 0.0028236447596579556, 'lgbm_num_leaves': 83}.\n",
            "<ipython-input-12-0edf83b10153>:33: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('xgb_learning_rate', 1e-3, 0.1),\n",
            "<ipython-input-12-0edf83b10153>:40: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('lgbm_learning_rate', 1e-3, 0.1),\n",
            "[I 2024-09-19 16:31:34,369] Trial 92 finished with values: [0.7984466153640881, 0.6288659793814433] and parameters: {'rf_n_estimators': 160, 'rf_max_depth': 10, 'rf_min_samples_split': 2, 'xgb_n_estimators': 270, 'xgb_learning_rate': 0.021983036658111597, 'xgb_max_depth': 10, 'lgbm_n_estimators': 790, 'lgbm_learning_rate': 0.04482622184329333, 'lgbm_num_leaves': 104}.\n",
            "<ipython-input-12-0edf83b10153>:33: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('xgb_learning_rate', 1e-3, 0.1),\n",
            "<ipython-input-12-0edf83b10153>:40: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('lgbm_learning_rate', 1e-3, 0.1),\n",
            "[I 2024-09-19 16:31:54,622] Trial 93 finished with values: [0.8030787977384855, 0.5923149015932521] and parameters: {'rf_n_estimators': 205, 'rf_max_depth': 9, 'rf_min_samples_split': 10, 'xgb_n_estimators': 137, 'xgb_learning_rate': 0.002578545984610506, 'xgb_max_depth': 9, 'lgbm_n_estimators': 427, 'lgbm_learning_rate': 0.023686128936899326, 'lgbm_num_leaves': 113}.\n",
            "<ipython-input-12-0edf83b10153>:33: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('xgb_learning_rate', 1e-3, 0.1),\n",
            "<ipython-input-12-0edf83b10153>:40: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('lgbm_learning_rate', 1e-3, 0.1),\n",
            "[I 2024-09-19 16:32:10,881] Trial 94 finished with values: [0.808042137346659, 0.633552014995314] and parameters: {'rf_n_estimators': 216, 'rf_max_depth': 19, 'rf_min_samples_split': 10, 'xgb_n_estimators': 247, 'xgb_learning_rate': 0.09165825631360809, 'xgb_max_depth': 4, 'lgbm_n_estimators': 937, 'lgbm_learning_rate': 0.02606773469943641, 'lgbm_num_leaves': 39}.\n",
            "<ipython-input-12-0edf83b10153>:33: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('xgb_learning_rate', 1e-3, 0.1),\n",
            "<ipython-input-12-0edf83b10153>:40: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('lgbm_learning_rate', 1e-3, 0.1),\n",
            "[I 2024-09-19 16:32:28,267] Trial 95 finished with values: [0.7970633105113485, 0.6279287722586692] and parameters: {'rf_n_estimators': 294, 'rf_max_depth': 27, 'rf_min_samples_split': 8, 'xgb_n_estimators': 176, 'xgb_learning_rate': 0.0022976175646836888, 'xgb_max_depth': 7, 'lgbm_n_estimators': 624, 'lgbm_learning_rate': 0.054831396267441664, 'lgbm_num_leaves': 77}.\n",
            "<ipython-input-12-0edf83b10153>:33: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('xgb_learning_rate', 1e-3, 0.1),\n",
            "<ipython-input-12-0edf83b10153>:40: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('lgbm_learning_rate', 1e-3, 0.1),\n",
            "[I 2024-09-19 16:32:34,240] Trial 96 finished with values: [0.8049837556123917, 0.6082474226804123] and parameters: {'rf_n_estimators': 169, 'rf_max_depth': 18, 'rf_min_samples_split': 6, 'xgb_n_estimators': 212, 'xgb_learning_rate': 0.026500842465049607, 'xgb_max_depth': 8, 'lgbm_n_estimators': 165, 'lgbm_learning_rate': 0.007164503642525063, 'lgbm_num_leaves': 72}.\n",
            "<ipython-input-12-0edf83b10153>:33: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('xgb_learning_rate', 1e-3, 0.1),\n",
            "<ipython-input-12-0edf83b10153>:40: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('lgbm_learning_rate', 1e-3, 0.1),\n",
            "[I 2024-09-19 16:32:43,847] Trial 97 finished with values: [0.802155254870503, 0.6204311152764761] and parameters: {'rf_n_estimators': 237, 'rf_max_depth': 9, 'rf_min_samples_split': 10, 'xgb_n_estimators': 278, 'xgb_learning_rate': 0.05328113339271193, 'xgb_max_depth': 7, 'lgbm_n_estimators': 585, 'lgbm_learning_rate': 0.08806966626213401, 'lgbm_num_leaves': 35}.\n",
            "<ipython-input-12-0edf83b10153>:33: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('xgb_learning_rate', 1e-3, 0.1),\n",
            "<ipython-input-12-0edf83b10153>:40: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('lgbm_learning_rate', 1e-3, 0.1),\n",
            "[I 2024-09-19 16:33:04,258] Trial 98 finished with values: [0.8071885233120225, 0.6223055295220243] and parameters: {'rf_n_estimators': 197, 'rf_max_depth': 24, 'rf_min_samples_split': 9, 'xgb_n_estimators': 288, 'xgb_learning_rate': 0.01188635410462855, 'xgb_max_depth': 9, 'lgbm_n_estimators': 967, 'lgbm_learning_rate': 0.012795469472104713, 'lgbm_num_leaves': 52}.\n",
            "<ipython-input-12-0edf83b10153>:33: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('xgb_learning_rate', 1e-3, 0.1),\n",
            "<ipython-input-12-0edf83b10153>:40: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('lgbm_learning_rate', 1e-3, 0.1),\n",
            "[I 2024-09-19 16:33:12,502] Trial 99 finished with values: [0.7991611755117264, 0.5791940018744143] and parameters: {'rf_n_estimators': 230, 'rf_max_depth': 14, 'rf_min_samples_split': 6, 'xgb_n_estimators': 282, 'xgb_learning_rate': 0.013076184400307734, 'xgb_max_depth': 3, 'lgbm_n_estimators': 165, 'lgbm_learning_rate': 0.005599785495032869, 'lgbm_num_leaves': 97}.\n",
            "<ipython-input-12-0edf83b10153>:33: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('xgb_learning_rate', 1e-3, 0.1),\n",
            "<ipython-input-12-0edf83b10153>:40: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('lgbm_learning_rate', 1e-3, 0.1),\n",
            "[I 2024-09-19 16:33:24,131] Trial 100 finished with values: [0.8016070771194466, 0.598875351452671] and parameters: {'rf_n_estimators': 253, 'rf_max_depth': 25, 'rf_min_samples_split': 6, 'xgb_n_estimators': 212, 'xgb_learning_rate': 0.010009275928741177, 'xgb_max_depth': 8, 'lgbm_n_estimators': 165, 'lgbm_learning_rate': 0.007164503642525063, 'lgbm_num_leaves': 74}.\n",
            "<ipython-input-12-0edf83b10153>:33: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('xgb_learning_rate', 1e-3, 0.1),\n",
            "<ipython-input-12-0edf83b10153>:40: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('lgbm_learning_rate', 1e-3, 0.1),\n",
            "[I 2024-09-19 16:33:51,559] Trial 101 finished with values: [0.8015492049815051, 0.5895032802249297] and parameters: {'rf_n_estimators': 118, 'rf_max_depth': 5, 'rf_min_samples_split': 5, 'xgb_n_estimators': 292, 'xgb_learning_rate': 0.02154137631170516, 'xgb_max_depth': 5, 'lgbm_n_estimators': 899, 'lgbm_learning_rate': 0.002507700577226068, 'lgbm_num_leaves': 75}.\n",
            "<ipython-input-12-0edf83b10153>:33: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('xgb_learning_rate', 1e-3, 0.1),\n",
            "<ipython-input-12-0edf83b10153>:40: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('lgbm_learning_rate', 1e-3, 0.1),\n",
            "[I 2024-09-19 16:34:02,845] Trial 102 finished with values: [0.8057859277466357, 0.6204311152764761] and parameters: {'rf_n_estimators': 259, 'rf_max_depth': 9, 'rf_min_samples_split': 3, 'xgb_n_estimators': 217, 'xgb_learning_rate': 0.08318945911322717, 'xgb_max_depth': 8, 'lgbm_n_estimators': 488, 'lgbm_learning_rate': 0.01039914544555736, 'lgbm_num_leaves': 70}.\n",
            "<ipython-input-12-0edf83b10153>:33: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('xgb_learning_rate', 1e-3, 0.1),\n",
            "<ipython-input-12-0edf83b10153>:40: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('lgbm_learning_rate', 1e-3, 0.1),\n",
            "[I 2024-09-19 16:34:37,167] Trial 103 finished with values: [0.7969821287622917, 0.6326148078725399] and parameters: {'rf_n_estimators': 160, 'rf_max_depth': 19, 'rf_min_samples_split': 2, 'xgb_n_estimators': 114, 'xgb_learning_rate': 0.09165825631360809, 'xgb_max_depth': 10, 'lgbm_n_estimators': 790, 'lgbm_learning_rate': 0.04482622184329333, 'lgbm_num_leaves': 104}.\n",
            "<ipython-input-12-0edf83b10153>:33: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('xgb_learning_rate', 1e-3, 0.1),\n",
            "<ipython-input-12-0edf83b10153>:40: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('lgbm_learning_rate', 1e-3, 0.1),\n",
            "[I 2024-09-19 16:34:55,707] Trial 104 finished with values: [0.8044275400643989, 0.6316776007497656] and parameters: {'rf_n_estimators': 120, 'rf_max_depth': 24, 'rf_min_samples_split': 9, 'xgb_n_estimators': 166, 'xgb_learning_rate': 0.04553161280542373, 'xgb_max_depth': 7, 'lgbm_n_estimators': 967, 'lgbm_learning_rate': 0.04059040270910901, 'lgbm_num_leaves': 52}.\n",
            "<ipython-input-12-0edf83b10153>:33: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('xgb_learning_rate', 1e-3, 0.1),\n",
            "<ipython-input-12-0edf83b10153>:40: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('lgbm_learning_rate', 1e-3, 0.1),\n",
            "[I 2024-09-19 16:35:20,917] Trial 105 finished with values: [0.801224477985278, 0.6391752577319587] and parameters: {'rf_n_estimators': 197, 'rf_max_depth': 24, 'rf_min_samples_split': 9, 'xgb_n_estimators': 247, 'xgb_learning_rate': 0.09165825631360809, 'xgb_max_depth': 9, 'lgbm_n_estimators': 937, 'lgbm_learning_rate': 0.02606773469943641, 'lgbm_num_leaves': 52}.\n",
            "<ipython-input-12-0edf83b10153>:33: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('xgb_learning_rate', 1e-3, 0.1),\n",
            "<ipython-input-12-0edf83b10153>:40: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('lgbm_learning_rate', 1e-3, 0.1),\n",
            "[I 2024-09-19 16:35:38,423] Trial 106 finished with values: [0.7941970321238592, 0.5538894095595126] and parameters: {'rf_n_estimators': 235, 'rf_max_depth': 5, 'rf_min_samples_split': 8, 'xgb_n_estimators': 139, 'xgb_learning_rate': 0.0016470735397092588, 'xgb_max_depth': 8, 'lgbm_n_estimators': 561, 'lgbm_learning_rate': 0.0027279656438761453, 'lgbm_num_leaves': 72}.\n",
            "<ipython-input-12-0edf83b10153>:33: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('xgb_learning_rate', 1e-3, 0.1),\n",
            "<ipython-input-12-0edf83b10153>:40: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('lgbm_learning_rate', 1e-3, 0.1),\n",
            "[I 2024-09-19 16:35:49,977] Trial 107 finished with values: [0.8015636730159903, 0.5932521087160263] and parameters: {'rf_n_estimators': 237, 'rf_max_depth': 25, 'rf_min_samples_split': 10, 'xgb_n_estimators': 247, 'xgb_learning_rate': 0.003964021914859738, 'xgb_max_depth': 3, 'lgbm_n_estimators': 253, 'lgbm_learning_rate': 0.007085456815701984, 'lgbm_num_leaves': 91}.\n",
            "<ipython-input-12-0edf83b10153>:33: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('xgb_learning_rate', 1e-3, 0.1),\n",
            "<ipython-input-12-0edf83b10153>:40: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('lgbm_learning_rate', 1e-3, 0.1),\n",
            "[I 2024-09-19 16:36:14,276] Trial 108 finished with values: [0.8060238465359506, 0.626991565135895] and parameters: {'rf_n_estimators': 151, 'rf_max_depth': 19, 'rf_min_samples_split': 9, 'xgb_n_estimators': 247, 'xgb_learning_rate': 0.09165825631360809, 'xgb_max_depth': 4, 'lgbm_n_estimators': 937, 'lgbm_learning_rate': 0.020237228798527834, 'lgbm_num_leaves': 83}.\n",
            "<ipython-input-12-0edf83b10153>:33: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('xgb_learning_rate', 1e-3, 0.1),\n",
            "<ipython-input-12-0edf83b10153>:40: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('lgbm_learning_rate', 1e-3, 0.1),\n",
            "[I 2024-09-19 16:36:58,664] Trial 109 finished with values: [0.8016030582209782, 0.6307403936269915] and parameters: {'rf_n_estimators': 259, 'rf_max_depth': 29, 'rf_min_samples_split': 4, 'xgb_n_estimators': 217, 'xgb_learning_rate': 0.05328113339271193, 'xgb_max_depth': 10, 'lgbm_n_estimators': 953, 'lgbm_learning_rate': 0.007451861088267603, 'lgbm_num_leaves': 127}.\n",
            "<ipython-input-12-0edf83b10153>:33: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('xgb_learning_rate', 1e-3, 0.1),\n",
            "<ipython-input-12-0edf83b10153>:40: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('lgbm_learning_rate', 1e-3, 0.1),\n",
            "[I 2024-09-19 16:37:18,283] Trial 110 finished with values: [0.7981122430115374, 0.6307403936269915] and parameters: {'rf_n_estimators': 283, 'rf_max_depth': 11, 'rf_min_samples_split': 4, 'xgb_n_estimators': 278, 'xgb_learning_rate': 0.044782869792071814, 'xgb_max_depth': 7, 'lgbm_n_estimators': 953, 'lgbm_learning_rate': 0.08806966626213401, 'lgbm_num_leaves': 56}.\n",
            "<ipython-input-12-0edf83b10153>:33: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('xgb_learning_rate', 1e-3, 0.1),\n",
            "<ipython-input-12-0edf83b10153>:40: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('lgbm_learning_rate', 1e-3, 0.1),\n",
            "[I 2024-09-19 16:37:49,798] Trial 111 finished with values: [0.8042667841256725, 0.6176194939081537] and parameters: {'rf_n_estimators': 160, 'rf_max_depth': 29, 'rf_min_samples_split': 10, 'xgb_n_estimators': 217, 'xgb_learning_rate': 0.003113632997684942, 'xgb_max_depth': 7, 'lgbm_n_estimators': 790, 'lgbm_learning_rate': 0.009050350510876399, 'lgbm_num_leaves': 111}.\n",
            "<ipython-input-12-0edf83b10153>:33: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('xgb_learning_rate', 1e-3, 0.1),\n",
            "<ipython-input-12-0edf83b10153>:40: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('lgbm_learning_rate', 1e-3, 0.1),\n",
            "[I 2024-09-19 16:37:56,741] Trial 112 finished with values: [0.8052723125224053, 0.6307403936269915] and parameters: {'rf_n_estimators': 259, 'rf_max_depth': 29, 'rf_min_samples_split': 10, 'xgb_n_estimators': 278, 'xgb_learning_rate': 0.08318945911322717, 'xgb_max_depth': 7, 'lgbm_n_estimators': 241, 'lgbm_learning_rate': 0.00963597391061113, 'lgbm_num_leaves': 56}.\n",
            "<ipython-input-12-0edf83b10153>:33: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('xgb_learning_rate', 1e-3, 0.1),\n",
            "<ipython-input-12-0edf83b10153>:40: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('lgbm_learning_rate', 1e-3, 0.1),\n",
            "[I 2024-09-19 16:38:04,545] Trial 113 finished with values: [0.8006537944027997, 0.5951265229615745] and parameters: {'rf_n_estimators': 193, 'rf_max_depth': 30, 'rf_min_samples_split': 6, 'xgb_n_estimators': 212, 'xgb_learning_rate': 0.002578545984610506, 'xgb_max_depth': 6, 'lgbm_n_estimators': 165, 'lgbm_learning_rate': 0.007164503642525063, 'lgbm_num_leaves': 74}.\n",
            "<ipython-input-12-0edf83b10153>:33: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('xgb_learning_rate', 1e-3, 0.1),\n",
            "<ipython-input-12-0edf83b10153>:40: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('lgbm_learning_rate', 1e-3, 0.1),\n",
            "[I 2024-09-19 16:38:22,700] Trial 114 finished with values: [0.7980222196858507, 0.6288659793814433] and parameters: {'rf_n_estimators': 283, 'rf_max_depth': 22, 'rf_min_samples_split': 3, 'xgb_n_estimators': 299, 'xgb_learning_rate': 0.093104403747103, 'xgb_max_depth': 4, 'lgbm_n_estimators': 579, 'lgbm_learning_rate': 0.08806966626213401, 'lgbm_num_leaves': 83}.\n",
            "<ipython-input-12-0edf83b10153>:33: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('xgb_learning_rate', 1e-3, 0.1),\n",
            "<ipython-input-12-0edf83b10153>:40: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('lgbm_learning_rate', 1e-3, 0.1),\n",
            "[I 2024-09-19 16:38:33,494] Trial 115 finished with values: [0.8065905112199607, 0.60543580131209] and parameters: {'rf_n_estimators': 280, 'rf_max_depth': 13, 'rf_min_samples_split': 6, 'xgb_n_estimators': 212, 'xgb_learning_rate': 0.051702729187721126, 'xgb_max_depth': 8, 'lgbm_n_estimators': 165, 'lgbm_learning_rate': 0.0027279656438761453, 'lgbm_num_leaves': 45}.\n",
            "<ipython-input-12-0edf83b10153>:33: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('xgb_learning_rate', 1e-3, 0.1),\n",
            "<ipython-input-12-0edf83b10153>:40: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('lgbm_learning_rate', 1e-3, 0.1),\n",
            "[I 2024-09-19 16:38:48,713] Trial 116 finished with values: [0.8002462780981285, 0.6241799437675727] and parameters: {'rf_n_estimators': 257, 'rf_max_depth': 18, 'rf_min_samples_split': 10, 'xgb_n_estimators': 123, 'xgb_learning_rate': 0.007533239785309867, 'xgb_max_depth': 7, 'lgbm_n_estimators': 866, 'lgbm_learning_rate': 0.06741163809401653, 'lgbm_num_leaves': 38}.\n",
            "<ipython-input-12-0edf83b10153>:33: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('xgb_learning_rate', 1e-3, 0.1),\n",
            "<ipython-input-12-0edf83b10153>:40: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  'learning_rate': trial.suggest_loguniform('lgbm_learning_rate', 1e-3, 0.1),\n"
          ]
        }
      ],
      "source": [
        "import optuna\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
        "from sklearn.metrics import roc_auc_score, recall_score\n",
        "from xgboost import XGBClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 데이터 준비\n",
        "X = df.drop(['deposit', 'duration'], axis=1)\n",
        "df['deposit'] = df['deposit'].replace({'yes': 1, 'no': 0})\n",
        "y = df['deposit']\n",
        "\n",
        "# 범주형 변수 원-핫 인코딩\n",
        "X = pd.get_dummies(X, drop_first=True)\n",
        "\n",
        "# 데이터 분할\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "def objective(trial):\n",
        "    # RandomForest 파라미터\n",
        "    rf_params = {\n",
        "        'n_estimators': trial.suggest_int('rf_n_estimators', 100, 300),\n",
        "        'max_depth': trial.suggest_int('rf_max_depth', 5, 30),\n",
        "        'min_samples_split': trial.suggest_int('rf_min_samples_split', 2, 10)\n",
        "    }\n",
        "\n",
        "    # XGBoost 파라미터\n",
        "    xgb_params = {\n",
        "        'n_estimators': trial.suggest_int('xgb_n_estimators', 100, 300),\n",
        "        'learning_rate': trial.suggest_loguniform('xgb_learning_rate', 1e-3, 0.1),\n",
        "        'max_depth': trial.suggest_int('xgb_max_depth', 3, 10)\n",
        "    }\n",
        "\n",
        "    # LightGBM 파라미터\n",
        "    lgbm_params = {\n",
        "        'n_estimators': trial.suggest_int('lgbm_n_estimators', 100, 1000),\n",
        "        'learning_rate': trial.suggest_loguniform('lgbm_learning_rate', 1e-3, 0.1),\n",
        "        'num_leaves': trial.suggest_int('lgbm_num_leaves', 31, 127)\n",
        "    }\n",
        "\n",
        "    # 모델 생성\n",
        "    rf = RandomForestClassifier(**rf_params, random_state=42)\n",
        "    xgb = XGBClassifier(**xgb_params, random_state=42)\n",
        "    lgbm = LGBMClassifier(**lgbm_params, random_state=42)\n",
        "\n",
        "    # Voting Classifier 생성\n",
        "    voting_clf = VotingClassifier([\n",
        "        ('rf', rf),\n",
        "        ('xgb', xgb),\n",
        "        ('lgbm', lgbm)\n",
        "    ], voting='soft', weights=[1, 1, 1])\n",
        "\n",
        "    # 모델 학습\n",
        "    voting_clf.fit(X_train, y_train)\n",
        "\n",
        "    # 예측\n",
        "    y_pred = voting_clf.predict(X_test)\n",
        "    y_pred_proba = voting_clf.predict_proba(X_test)[:, 1]\n",
        "\n",
        "    # AUC와 Recall 계산\n",
        "    auc = roc_auc_score(y_test, y_pred_proba)\n",
        "    recall = recall_score(y_test, y_pred)\n",
        "\n",
        "    return auc, recall\n",
        "\n",
        "# Optuna 스터디 생성 및 최적화 실행\n",
        "study = optuna.create_study(\n",
        "    study_name=\"bank_marketing_voting_clf\",\n",
        "    storage=\"sqlite:///optuna_results.db\",\n",
        "    directions=[\"maximize\", \"maximize\"],\n",
        "    load_if_exists=True\n",
        ")\n",
        "\n",
        "study.optimize(objective, n_trials=50)\n",
        "\n",
        "# 최적의 파라미터 출력\n",
        "best_trial = study.best_trials[0]  # Pareto optimal solutions의 첫 번째 trial\n",
        "print(\"Best trial:\")\n",
        "print(\"  AUC: \", best_trial.values[0])\n",
        "print(\"  Recall: \", best_trial.values[1])\n",
        "print(\"  Params: \")\n",
        "for key, value in best_trial.params.items():\n",
        "    print(\"    {}: {}\".format(key, value))\n",
        "\n",
        "# 최적의 파라미터로 모델 재학습\n",
        "best_params = best_trial.params\n",
        "rf = RandomForestClassifier(\n",
        "    n_estimators=best_params['rf_n_estimators'],\n",
        "    max_depth=best_params['rf_max_depth'],\n",
        "    min_samples_split=best_params['rf_min_samples_split'],\n",
        "    random_state=42\n",
        ")\n",
        "xgb = XGBClassifier(\n",
        "    n_estimators=best_params['xgb_n_estimators'],\n",
        "    learning_rate=best_params['xgb_learning_rate'],\n",
        "    max_depth=best_params['xgb_max_depth'],\n",
        "    random_state=42\n",
        ")\n",
        "lgbm = LGBMClassifier(\n",
        "    n_estimators=best_params['lgbm_n_estimators'],\n",
        "    learning_rate=best_params['lgbm_learning_rate'],\n",
        "    num_leaves=best_params['lgbm_num_leaves'],\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "best_voting_clf = VotingClassifier([\n",
        "    ('rf', rf),\n",
        "    ('xgb', xgb),\n",
        "    ('lgbm', lgbm)\n",
        "], voting='soft', weights=[1, 1, 1])\n",
        "\n",
        "best_voting_clf.fit(X_train, y_train)\n",
        "\n",
        "# 최종 성능 평가\n",
        "y_pred = best_voting_clf.predict(X_test)\n",
        "y_pred_proba = best_voting_clf.predict_proba(X_test)[:, 1]\n",
        "\n",
        "final_auc = roc_auc_score(y_test, y_pred_proba)\n",
        "final_recall = recall_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Final Soft Voting Results:\")\n",
        "print(f\"AUC: {final_auc:.4f}\")\n",
        "print(f\"Recall: {final_recall:.4f}\")\n",
        "\n",
        "# 결과 시각화\n",
        "def plot_optimization_history(study):\n",
        "    fig = optuna.visualization.plot_optimization_history(study)\n",
        "    fig.show()\n",
        "\n",
        "def plot_param_importances(study):\n",
        "    fig = optuna.visualization.plot_param_importances(study)\n",
        "    fig.show()\n",
        "\n",
        "def plot_pareto_front(study):\n",
        "    fig = optuna.visualization.plot_pareto_front(study)\n",
        "    fig.show()\n",
        "\n",
        "plot_optimization_history(study)\n",
        "plot_param_importances(study)\n",
        "plot_pareto_front(study)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQ47q9MvHXzu"
      },
      "source": [
        "# pycaret 안됨 issue -> 이후부터 pycaret (lgmg)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hbFrla4V8h7g"
      },
      "source": [
        "#pycaret , AutoML\n",
        "\n",
        "lightgbm\tLight Gradient Boosting Machine\t0.7369\t0.7930\t0.7369\t0.7424\t0.7335\t0.4675\t0.4753\t1.1510\n",
        "\n",
        "gbc\tGradient Boosting Classifier\t0.7345\t0.7914\t0.7345\t0.7416\t0.7302\t0.4618\t0.4717\t2.0840\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q_kVN5_w1WmL"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5-HKw1Im492T"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "75aQcb3FIobw"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fDcBdxovgpMH"
      },
      "outputs": [],
      "source": [
        "\n",
        "df=pd.read_csv('/content/bank.csv')\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y_PhL67DcWiA"
      },
      "outputs": [],
      "source": [
        "best_model = create_model(best_models[0])\n",
        "tuned_model = tune_model(best_model, optimize='AUC')\n",
        "\n",
        "\n",
        "evaluate_model(tuned_model)\n",
        "plot_model(tuned_model, plot='auc')\n",
        "plot_model(tuned_model, plot='confusion_matrix')\n",
        "plot_model(tuned_model, plot='feature_importance')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DgioJl4QYA1d"
      },
      "outputs": [],
      "source": [
        "# 필요한 라이브러리 설치\n",
        "!pip install pycaret pandas matplotlib seaborn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fkd-Z2I_WP02"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# 데이터 처리 및 모델 훈련 코드\n",
        "import pandas as pd\n",
        "from pycaret.classification import *\n",
        "\n",
        "# 데이터 로드\n",
        "data = df\n",
        "\n",
        "# 데이터프레임으로 변환\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# PyCaret 설정\n",
        "# duration 변수를 제거한 상태로 모델링 진행\n",
        "clf = setup(data=df,\n",
        "            target='deposit',\n",
        "            ignore_features=['duration'],  # duration 변수를 feature selection에서 제외\n",
        "            session_id=123,\n",
        "            # silent=True,\n",
        "            use_gpu=False)\n",
        "\n",
        "# 상위 3개의 모델 훈련 및 비교\n",
        "top3_models = compare_models(n_select=3)\n",
        "\n",
        "# 상위 모델들의 성능 확인\n",
        "print(top3_models)\n",
        "\n",
        "# # 각 모델별 시각화\n",
        "# plot_model(top3_models[0], plot='auc')  # 첫 번째 모델의 AUC 곡선 시각화\n",
        "# plot_model(top3_models[1], plot='confusion_matrix')  # 두 번째 모델의 혼동 행렬 시각화\n",
        "# plot_model(top3_models[2], plot='feature')  # 세 번째 모델의 feature 중요도 시각화\n",
        "\n",
        "# 모델 블렌딩 (앙상블)\n",
        "blended_model = blend_models(estimator_list=top3_models)\n",
        "\n",
        "# 블렌딩된 모델 평가\n",
        "evaluate_model(blended_model)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0-1xNnhg8aL_"
      },
      "source": [
        "# optuna 사용\n",
        "\n",
        "basemodel: Light Gradient Boosting Machine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2JD7-Q_H-h88"
      },
      "outputs": [],
      "source": [
        "! pip install optuna"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4sSWVVNg2Dyz"
      },
      "outputs": [],
      "source": [
        "import optuna\n",
        "import lightgbm as lgb\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, recall_score, roc_auc_score\n",
        "from lightgbm import early_stopping, log_evaluation\n",
        "\n",
        "# 데이터 로드 및 전처리 (이전 코드와 동일)\n",
        "df = pd.read_csv('/content/bank.csv')\n",
        "\n",
        "def prepare_data(data, target):\n",
        "    le = LabelEncoder()\n",
        "    data[target] = le.fit_transform(data[target])\n",
        "    return data, le\n",
        "\n",
        "df, label_encoder = prepare_data(df, 'deposit')\n",
        "\n",
        "X = df.drop(['deposit', 'duration'], axis=1)\n",
        "y = df['deposit']\n",
        "\n",
        "for col in X.select_dtypes(include=['object']):\n",
        "    X[col] = LabelEncoder().fit_transform(X[col])\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Optuna 최적화 함수 (이전과 동일)\n",
        "def objective(trial):\n",
        "    params = {\n",
        "        'objective': 'binary',\n",
        "        'metric': 'auc',\n",
        "        'num_leaves': trial.suggest_int('num_leaves', 20, 3000),\n",
        "        'max_depth': trial.suggest_int('max_depth', 3, 12),\n",
        "        'learning_rate': trial.suggest_loguniform('learning_rate', 1e-3, 1.0),\n",
        "        'n_estimators': trial.suggest_int('n_estimators', 50, 10000),\n",
        "        'min_child_samples': trial.suggest_int('min_child_samples', 1, 300),\n",
        "        'subsample': trial.suggest_uniform('subsample', 0.6, 1.0),\n",
        "        'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.6, 1.0),\n",
        "        'reg_alpha': trial.suggest_loguniform('reg_alpha', 1e-8, 10.0),\n",
        "        'reg_lambda': trial.suggest_loguniform('reg_lambda', 1e-8, 10.0),\n",
        "        'random_state': 42\n",
        "    }\n",
        "\n",
        "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "    cv_scores = []\n",
        "\n",
        "    for train_idx, val_idx in cv.split(X_train, y_train):\n",
        "        X_train_fold, X_val_fold = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
        "        y_train_fold, y_val_fold = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
        "\n",
        "        train_data = lgb.Dataset(X_train_fold, label=y_train_fold)\n",
        "        val_data = lgb.Dataset(X_val_fold, label=y_val_fold)\n",
        "\n",
        "        model = lgb.train(\n",
        "            params,\n",
        "            train_data,\n",
        "            valid_sets=[val_data],\n",
        "            num_boost_round=params['n_estimators'],\n",
        "            callbacks=[early_stopping(100), log_evaluation(period=0)]\n",
        "        )\n",
        "\n",
        "        cv_scores.append(model.best_score['valid_0']['auc'])\n",
        "\n",
        "    return np.mean(cv_scores)\n",
        "\n",
        "# SQLite 데이터베이스를 사용하여 Optuna 연구 생성\n",
        "study_name = \"lightgbm_optimization\"\n",
        "storage_name = \"sqlite:///optuna_studies.db\"\n",
        "study = optuna.create_study(study_name=study_name, storage=storage_name, direction='maximize', load_if_exists=True)\n",
        "\n",
        "# 최적화 실행\n",
        "study.optimize(objective, n_trials=50, n_jobs=-1, timeout=3600)\n",
        "\n",
        "# 최적의 하이퍼파라미터 출력\n",
        "print(\"Best trial:\")\n",
        "trial = study.best_trial\n",
        "print(\"  Value: \", trial.value)\n",
        "print(\"  Params: \")\n",
        "for key, value in trial.params.items():\n",
        "    print(\"    {}: {}\".format(key, value))\n",
        "\n",
        "# 최적화된 모델 생성 및 훈련\n",
        "best_params = study.best_params\n",
        "best_params['objective'] = 'binary'\n",
        "best_params['metric'] = 'auc'\n",
        "\n",
        "train_data = lgb.Dataset(X_train, label=y_train)\n",
        "final_model = lgb.train(best_params, train_data, num_boost_round=best_params['n_estimators'])\n",
        "\n",
        "# 테스트 세트에 대한 예측 및 성능 평가\n",
        "y_pred = final_model.predict(X_test)\n",
        "y_pred_binary = (y_pred > 0.5).astype(int)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred_binary)\n",
        "recall = recall_score(y_test, y_pred_binary)\n",
        "auc = roc_auc_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"AUC: {auc:.4f}\")\n",
        "\n",
        "# 특성 중요도 출력\n",
        "feature_importance = pd.DataFrame({\n",
        "    'feature': X.columns,\n",
        "    'importance': final_model.feature_importance()\n",
        "}).sort_values('importance', ascending=False)\n",
        "\n",
        "print(\"\\nTop 10 Important Features:\")\n",
        "print(feature_importance.head(10))\n",
        "\n",
        "# 레이블 디코딩 함수\n",
        "def decode_predictions(predictions):\n",
        "    return label_encoder.inverse_transform(predictions)\n",
        "\n",
        "# 디코딩된 예측 결과 예시\n",
        "decoded_predictions = decode_predictions(y_pred_binary)\n",
        "print(\"\\nSample of decoded predictions:\", decoded_predictions[:10])\n",
        "\n",
        "# 모든 시도의 결과 확인\n",
        "print(\"\\nAll trials:\")\n",
        "trial_data = study.trials_dataframe()\n",
        "print(trial_data.sort_values('value', ascending=False).head())\n",
        "\n",
        "# 가장 좋은 시도의 하이퍼파라미터 저장\n",
        "best_params_df = pd.DataFrame([study.best_params])\n",
        "best_params_df.to_csv('best_lightgbm_params.csv', index=False)\n",
        "print(\"\\nBest parameters saved to 'best_lightgbm_params.csv'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7A4nzBJ38suy"
      },
      "source": [
        "# featur selection + xgboost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kkmvP5D5zvmx"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score, recall_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "\n",
        "# 데이터 로드\n",
        "df = pd.read_csv('/content/bank.csv')\n",
        "\n",
        "# 전처리\n",
        "le = LabelEncoder()\n",
        "for column in df.select_dtypes(include=['object']).columns:\n",
        "    df[column] = le.fit_transform(df[column])\n",
        "\n",
        "# 특성과 타겟 분리\n",
        "X = df.drop(['deposit', 'duration'], axis=1)\n",
        "y = df['deposit']\n",
        "\n",
        "# 훈련 및 테스트 세트 분할\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 초기 XGBoost 모델 생성\n",
        "initial_model = XGBClassifier(random_state=42)\n",
        "initial_model.fit(X_train, y_train)\n",
        "\n",
        "# 특성 중요도 기반 특성 선택\n",
        "selector = SelectFromModel(initial_model, prefit=True, threshold='median')\n",
        "X_train_selected = selector.transform(X_train)\n",
        "X_test_selected = selector.transform(X_test)\n",
        "\n",
        "# 선택된 특성으로 새 XGBoost 모델 훈련\n",
        "model = XGBClassifier(random_state=42)\n",
        "\n",
        "# 하이퍼파라미터 그리드 정의\n",
        "param_grid = {\n",
        "    'max_depth': [3, 5, 7],\n",
        "    'learning_rate': [0.01, 0.1, 0.3],\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'min_child_weight': [1, 3, 5],\n",
        "    'subsample': [0.8, 0.9, 1.0],\n",
        "    'colsample_bytree': [0.8, 0.9, 1.0]\n",
        "}\n",
        "\n",
        "# GridSearchCV를 사용한 하이퍼파라미터 튜닝\n",
        "grid_search = GridSearchCV(estimator=model, param_grid=param_grid,\n",
        "                           cv=3, scoring='recall', n_jobs=-1)\n",
        "grid_search.fit(X_train_selected, y_train)\n",
        "\n",
        "# 최적의 모델 선택\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# 테스트 세트에 대한 예측\n",
        "y_pred = best_model.predict(X_test_selected)\n",
        "\n",
        "# 정확도와 재현율 계산\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred, pos_label=1)\n",
        "\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "\n",
        "# 특성 중요도 출력\n",
        "feature_importance = best_model.feature_importances_\n",
        "feature_names = X.columns[selector.get_support()]\n",
        "for name, importance in zip(feature_names, feature_importance):\n",
        "    print(f\"{name}: {importance}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GCiPguJvDyxz"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
